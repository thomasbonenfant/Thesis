@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  year = {2018 }
}

@PhdThesis{Watkins:1989,
  author =       "Watkins, Christopher John Cornish Hellaby",
  title =        "Learning from Delayed Rewards",
  school =       "King's College",
  year =         "1989",
  address =   "Cambridge, UK",
  month =     "May",
  url = "http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf",
  bib2html_rescat = "Parameter",
}
@article{DQN,
  author       = {Volodymyr Mnih and
                  Koray Kavukcuoglu and
                  David Silver and
                  Alex Graves and
                  Ioannis Antonoglou and
                  Daan Wierstra and
                  Martin A. Riedmiller},
  title        = {Playing Atari with Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1312.5602},
  year         = {2013},
  url          = {http://arxiv.org/abs/1312.5602},
  eprinttype    = {arXiv},
  eprint       = {1312.5602},
  timestamp    = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{SAC,
  author       = {Tuomas Haarnoja and
                  Aurick Zhou and
                  Pieter Abbeel and
                  Sergey Levine},
  title        = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
                  with a Stochastic Actor},
  journal      = {CoRR},
  volume       = {abs/1801.01290},
  year         = {2018},
  url          = {http://arxiv.org/abs/1801.01290},
  eprinttype    = {arXiv},
  eprint       = {1801.01290},
  timestamp    = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{SAC_modified,
  author       = {Tuomas Haarnoja and
                  Aurick Zhou and
                  Kristian Hartikainen and
                  George Tucker and
                  Sehoon Ha and
                  Jie Tan and
                  Vikash Kumar and
                  Henry Zhu and
                  Abhishek Gupta and
                  Pieter Abbeel and
                  Sergey Levine},
  title        = {Soft Actor-Critic Algorithms and Applications},
  journal      = {CoRR},
  volume       = {abs/1812.05905},
  year         = {2018},
  url          = {http://arxiv.org/abs/1812.05905},
  eprinttype    = {arXiv},
  eprint       = {1812.05905},
  timestamp    = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1812-05905.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Hasselt2010,
 author = {Hasselt, Hado},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Double Q-learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf},
 volume = {23},
 year = {2010}
}

@misc{Fujimoto2018,
      title={Addressing Function Approximation Error in Actor-Critic Methods}, 
      author={Scott Fujimoto and Herke van Hoof and David Meger},
      year={2018},
      eprint={1802.09477},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@Article{Kober2013,
  author  = {Kober, Jens AND Bagnell, J. Andrew AND Peters, Jan},
  title   = {Reinforcement Learning in Robotics: A Survey},
  journal = {International Journal of Robotics Research},
  year    = {2013},
  volume  = {32},
  number  = {11},
  pages   = {1238--1274},
  doi     = {10.1177/0278364913495721},
  file    = {http://www.jenskober.de/publications/Kober2013IJRR.pdf},
  oa      = {green},
}

@inproceedings{Atacom,
  title     = {Robot Reinforcement Learning on the Constraint Manifold},
  author    = {Liu, Puze and Tateo, Davide and Bou-Ammar, Haitham and Peters, Jan},
  booktitle   = {Proceedings of the 5th Conference on Robot Learning},
  pages       = {1357--1366},
  year        = {2022},
  editor      = {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
  volume      = {164},
  series      = {Proceedings of Machine Learning Research},
  publisher   = {PMLR},
}

@phdthesis{Laud2004,
author = {Laud, Adam Daniel},
advisor = {Dejong, Gerald},
title = {Theory and application of reward shaping in reinforcement learning},
year = {2004},
publisher = {University of Illinois at Urbana-Champaign},
address = {USA},
abstract = {Applying conventional reinforcement to complex domains requires the use of an overly simplified task model, or a large amount of training experience. This problem results from the need to experience everything about an environment before gaining confidence in a course of action. But for most interesting problems, the domain is far too large to be exhaustively explored. We address this disparity with reward shapingâ€”a technique that provides localized feedback based on prior knowledge to guide the learning process. By using localized advice, learning is focused into the most relevant areas, which allows for efficient optimization, even in complex domains. We propose a complete theory for the process of reward shaping that demonstrates how it accelerates learning, what the ideal shaping rewards are like, and how to express prior knowledge in order to enhance the learning process. Central to our analysis is the idea of the reward horizon, which characterizes the delay between an action and accurate estimation of its value. In order to maintain focused learning, the goal of reward shaping is to promote a low reward horizon. One type of reward that always generates a low reward horizon is opportunity value. Opportunity value is the value for choosing one action rather than doing nothing. This information, when combined with the native rewards, is enough to decide the best action immediately. Using opportunity value as a model, we suggest subgoal shaping and dynamic shaping as techniques to communicate whatever prior knowledge is available. We demonstrate our theory with two applications: a stochastic gridworld, and a bipedal walking control task. In all cases, the experiments uphold the analytical predictions; most notably that reducing the reward horizon implies faster learning. The bipedal walking task demonstrates that our reward shaping techniques allow a conventional reinforcement learning algorithm to find a good behavior efficiently despite a large state space with stochastic actions.},
note = {AAI3130966}
}

@misc{air_hockey_competition,
    title        = {Air Hockey Challenge '23},
    url          = {https://air-hockey-challenge.robot-learning.net/},
}

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  year = {2018 }
}

@PhdThesis{Watkins:1989,
  author =       "Watkins, Christopher John Cornish Hellaby",
  title =        "Learning from Delayed Rewards",
  school =       "King's College",
  year =         "1989",
  address =   "Cambridge, UK",
  month =     "May",
  url = "http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf",
  bib2html_rescat = "Parameter",
}
@article{DQN,
  author       = {Volodymyr Mnih and
                  Koray Kavukcuoglu and
                  David Silver and
                  Alex Graves and
                  Ioannis Antonoglou and
                  Daan Wierstra and
                  Martin A. Riedmiller},
  title        = {Playing Atari with Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1312.5602},
  year         = {2013},
  url          = {http://arxiv.org/abs/1312.5602},
  eprinttype    = {arXiv},
  eprint       = {1312.5602},
  timestamp    = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{SAC,
  author       = {Tuomas Haarnoja and
                  Aurick Zhou and
                  Pieter Abbeel and
                  Sergey Levine},
  title        = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
                  with a Stochastic Actor},
  journal      = {CoRR},
  volume       = {abs/1801.01290},
  year         = {2018},
  url          = {http://arxiv.org/abs/1801.01290},
  eprinttype    = {arXiv},
  eprint       = {1801.01290},
  timestamp    = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{SAC_modified,
  author       = {Tuomas Haarnoja and
                  Aurick Zhou and
                  Kristian Hartikainen and
                  George Tucker and
                  Sehoon Ha and
                  Jie Tan and
                  Vikash Kumar and
                  Henry Zhu and
                  Abhishek Gupta and
                  Pieter Abbeel and
                  Sergey Levine},
  title        = {Soft Actor-Critic Algorithms and Applications},
  journal      = {CoRR},
  volume       = {abs/1812.05905},
  year         = {2018},
  url          = {http://arxiv.org/abs/1812.05905},
  eprinttype    = {arXiv},
  eprint       = {1812.05905},
  timestamp    = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1812-05905.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Hasselt2010,
 author = {Hasselt, Hado},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Double Q-learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf},
 volume = {23},
 year = {2010}
}

@misc{Fujimoto2018,
      title={Addressing Function Approximation Error in Actor-Critic Methods}, 
      author={Scott Fujimoto and Herke van Hoof and David Meger},
      year={2018},
      eprint={1802.09477},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@Article{Kober2013,
  author  = {Kober, Jens AND Bagnell, J. Andrew AND Peters, Jan},
  title   = {Reinforcement Learning in Robotics: A Survey},
  journal = {International Journal of Robotics Research},
  year    = {2013},
  volume  = {32},
  number  = {11},
  pages   = {1238--1274},
  doi     = {10.1177/0278364913495721},
  file    = {http://www.jenskober.de/publications/Kober2013IJRR.pdf},
  oa      = {green},
}

@inproceedings{Atacom,
  title     = {Robot Reinforcement Learning on the Constraint Manifold},
  author    = {Liu, Puze and Tateo, Davide and Bou-Ammar, Haitham and Peters, Jan},
  booktitle   = {Proceedings of the 5th Conference on Robot Learning},
  pages       = {1357--1366},
  year        = {2022},
  editor      = {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
  volume      = {164},
  series      = {Proceedings of Machine Learning Research},
  publisher   = {PMLR},
}

@phdthesis{Laud2004,
author = {Laud, Adam Daniel},
advisor = {Dejong, Gerald},
title = {Theory and application of reward shaping in reinforcement learning},
year = {2004},
publisher = {University of Illinois at Urbana-Champaign},
address = {USA},
abstract = {Applying conventional reinforcement to complex domains requires the use of an overly simplified task model, or a large amount of training experience. This problem results from the need to experience everything about an environment before gaining confidence in a course of action. But for most interesting problems, the domain is far too large to be exhaustively explored. We address this disparity with reward shaping—a technique that provides localized feedback based on prior knowledge to guide the learning process. By using localized advice, learning is focused into the most relevant areas, which allows for efficient optimization, even in complex domains. We propose a complete theory for the process of reward shaping that demonstrates how it accelerates learning, what the ideal shaping rewards are like, and how to express prior knowledge in order to enhance the learning process. Central to our analysis is the idea of the reward horizon, which characterizes the delay between an action and accurate estimation of its value. In order to maintain focused learning, the goal of reward shaping is to promote a low reward horizon. One type of reward that always generates a low reward horizon is opportunity value. Opportunity value is the value for choosing one action rather than doing nothing. This information, when combined with the native rewards, is enough to decide the best action immediately. Using opportunity value as a model, we suggest subgoal shaping and dynamic shaping as techniques to communicate whatever prior knowledge is available. We demonstrate our theory with two applications: a stochastic gridworld, and a bipedal walking control task. In all cases, the experiments uphold the analytical predictions; most notably that reducing the reward horizon implies faster learning. The bipedal walking task demonstrates that our reward shaping techniques allow a conventional reinforcement learning algorithm to find a good behavior efficiently despite a large state space with stochastic actions.},
note = {AAI3130966}
}

@misc{PPO,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@article{Options,
title = {Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
journal = {Artificial Intelligence},
volume = {112},
number = {1},
pages = {181-211},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00052-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000521},
author = {Richard S. Sutton and Doina Precup and Satinder Singh},
keywords = {Temporal abstraction, Reinforcement learning, Markov decision processes, Options, Macros, Macroactions, Subgoals, Intra-option learning, Hierarchical planning, Semi-Markov decision processes},
abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.}
}

@Article{survey_mdpi,
AUTHOR = {Hutsebaut-Buysse, Matthias and Mets, Kevin and Latré, Steven},
TITLE = {Hierarchical Reinforcement Learning: A Survey and Open Research Challenges},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {4},
YEAR = {2022},
NUMBER = {1},
PAGES = {172--221},
URL = {https://www.mdpi.com/2504-4990/4/1/9},
ISSN = {2504-4990},
ABSTRACT = {Reinforcement learning (RL) allows an agent to solve sequential decision-making problems by interacting with an environment in a trial-and-error fashion. When these environments are very complex, pure random exploration of possible solutions often fails, or is very sample inefficient, requiring an unreasonable amount of interaction with the environment. Hierarchical reinforcement learning (HRL) utilizes forms of temporal- and state-abstractions in order to tackle these challenges, while simultaneously paving the road for behavior reuse and increased interpretability of RL systems. In this survey paper we first introduce a selection of problem-specific approaches, which provided insight in how to utilize often handcrafted abstractions in specific task settings. We then introduce the Options framework, which provides a more generic approach, allowing abstractions to be discovered and learned semi-automatically. Afterwards we introduce the goal-conditional approach, which allows sub-behaviors to be embedded in a continuous space. In order to further advance the development of HRL agents, capable of simultaneously learning abstractions and how to use them, solely from interaction with complex high dimensional environments, we also identify a set of promising research directions.},
DOI = {10.3390/make4010009}
}

@inproceedings{motion_templates,
author = {Neumann, Gerhard and Maass, Wolfgang and Peters, Jan},
title = {Learning complex motions by sequencing simpler motion templates},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553471},
doi = {10.1145/1553374.1553471},
abstract = {Abstraction of complex, longer motor tasks into simpler elemental movements enables humans and animals to exhibit motor skills which have not yet been matched by robots. Humans intuitively decompose complex motions into smaller, simpler segments. For example when describing simple movements like drawing a triangle with a pen, we can easily name the basic steps of this movement.Surprisingly, such abstractions have rarely been used in artificial motor skill learning algorithms. These algorithms typically choose a new action (such as a torque or a force) at a very fast time-scale. As a result, both policy and temporal credit assignment problem become unnecessarily complex - often beyond the reach of current machine learning methods.We introduce a new framework for temporal abstractions in reinforcement learning (RL), i.e. RL with motion templates. We present a new algorithm for this framework which can learn high-quality policies by making only few abstract decisions.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {753–760},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@misc{option-critic,
      title={The Option-Critic Architecture}, 
      author={Pierre-Luc Bacon and Jean Harb and Doina Precup},
      year={2016},
      eprint={1609.05140},
      archivePrefix={arXiv},
      primaryClass={id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'}
}

@misc{PPOC,
      title={Learnings Options End-to-End for Continuous Action Tasks}, 
      author={Martin Klissarov and Pierre-Luc Bacon and Jean Harb and Doina Precup},
      year={2017},
      eprint={1712.00004},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{FuN,
      title={FeUdal Networks for Hierarchical Reinforcement Learning}, 
      author={Alexander Sasha Vezhnevets and Simon Osindero and Tom Schaul and Nicolas Heess and Max Jaderberg and David Silver and Koray Kavukcuoglu},
      year={2017},
      eprint={1703.01161},
      archivePrefix={arXiv},
      primaryClass={id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'}
}

@misc{HIRO,
      title={Data-Efficient Hierarchical Reinforcement Learning}, 
      author={Ofir Nachum and Shixiang Gu and Honglak Lee and Sergey Levine},
      year={2018},
      eprint={1805.08296},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@inproceedings{Horde,
author = {Sutton, Richard S. and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M. and White, Adam and Precup, Doina},
title = {Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction},
year = {2011},
isbn = {0982657161},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other artificial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a single predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the system's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, reward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever actions are taken by the system as a whole. Gradient-based temporal-difference learning methods are used to learn efficiently and reliably with function approximation in this off-policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real-time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from off-policy experience. Horde is a significant incremental step towards a real-time architecture for efficient learning of general knowledge from unsupervised sensorimotor interaction.},
booktitle = {The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {761–768},
numpages = {8},
keywords = {value function approximation, temporal-difference learning, robotics, reinforcement learning, real-time, off-policy learning, knowledge representation, artificial intelligence},
location = {Taipei, Taiwan},
series = {AAMAS '11}
}
@inproceedings{Feudal_rl,
 author = {Dayan, Peter and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Feudal Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d14220ee66aeec73c49038385428ec4c-Paper.pdf},
 volume = {5},
 year = {1992}
}

@article{mushroom_rl,
  author  = {Carlo D'Eramo and Davide Tateo and Andrea Bonarini and Marcello Restelli and Jan Peters},
  title   = {MushroomRL: Simplifying Reinforcement Learning Research},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {131},
  pages   = {1--5},
  url     = {http://jmlr.org/papers/v22/18-056.html}
}

@INPROCEEDINGS{MuJoCo,
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  keywords={Engines;Optimization;Computational modeling;Heuristic algorithms;Dynamics;Mathematical model},
  doi={10.1109/IROS.2012.6386109}}

@article{kalman_filter,
  title={An introduction to the Kalman filter},
  author={Welch, Greg and Bishop, Gary and others},
  year={1995},
  publisher={Chapel Hill, NC, USA}
}

@INPROCEEDINGS{baseline,
  author={Liu, Puze and Tateo, Davide and Bou-Ammar, Haitham and Peters, Jan},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Efficient and Reactive Planning for High Speed Robot Air Hockey}, 
  year={2021},
  volume={},
  number={},
  pages={586-593},
  keywords={Uncertainty;Service robots;Games;Robot sensing systems;Trajectory;Task analysis;Robots},
  doi={10.1109/IROS51168.2021.9636263}}

@INPROCEEDINGS{manipulability,
author={Vahrenkamp, Nikolaus and Asfour, Tamim and Metta, Giorgio and Sandini, Giulio and Dillmann, Rüdiger},
booktitle={2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012)}, 
title={Manipulability analysis}, 
year={2012},
volume={},
number={},
pages={568-573},
keywords={Joints;Jacobian matrices;Manipulators;Indexes;Ellipsoids;Kinematics},
doi={10.1109/HUMANOIDS.2012.6651576}}

@misc{CL,
      title={Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey}, 
      author={Sanmit Narvekar and Bei Peng and Matteo Leonetti and Jivko Sinapov and Matthew E. Taylor and Peter Stone},
      year={2020},
      eprint={2003.04960},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2003.04960}, 
}

@article{PGPE,
title = {Parameter-exploring policy gradients},
journal = {Neural Networks},
volume = {23},
number = {4},
pages = {551-559},
year = {2010},
note = {The 18th International Conference on Artificial Neural Networks, ICANN 2008},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2009.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608009003220},
author = {Frank Sehnke and Christian Osendorfer and Thomas Rückstieß and Alex Graves and Jan Peters and Jürgen Schmidhuber},
keywords = {Policy gradients, Stochastic optimisation, Reinforcement learning, Robotics, Control},
abstract = {We present a model-free reinforcement learning method for partially observable Markov decision problems. Our method estimates a likelihood gradient by sampling directly in parameter space, which leads to lower variance gradient estimates than obtained by regular policy gradient methods. We show that for several complex control tasks, including robust standing with a humanoid robot, this method outperforms well-known algorithms from the fields of standard policy gradients, finite difference methods and population based heuristics. We also show that the improvement is largest when the parameter samples are drawn symmetrically. Lastly we analyse the importance of the individual components of our method by incrementally incorporating them into the other algorithms, and measuring the gain in performance after each step.}
}

@article{PGPE_Likmeta,
title = {Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103568},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103568},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304085},
author = {Amarildo Likmeta and Alberto Maria Metelli and Andrea Tirinzoni and Riccardo Giol and Marcello Restelli and Danilo Romano},
keywords = {Autonomous driving, Decision making, Interpretability, Reinforcement learning, Parameter-based exploration},
abstract = {The design of high-level decision-making systems is a topical problem in the field of autonomous driving. In this paper, we combine traditional rule-based strategies and reinforcement learning (RL) with the goal of achieving transparency and robustness. On the one hand, the use of handcrafted rule-based controllers allows for transparency, i.e., it is always possible to determine why a given decision was made, but they struggle to scale to complex driving scenarios, in which several objectives need to be considered. On the other hand, black-box RL approaches enable us to deal with more complex scenarios, but they are usually hardly interpretable. In this paper, we combine the best properties of these two worlds by designing parametric rule-based controllers, in which interpretable rules can be provided by domain experts and their parameters are learned via RL. After illustrating how to apply parameter-based RL methods (PGPE) to this setting, we present extensive numerical simulations in the highway and in two urban scenarios: intersection and roundabout. For each scenario, we show the formalization as an RL problem and we discuss the results of our approach in comparison with handcrafted rule-based controllers and black-box RL techniques.}
}
\section{Reinforcement Learning}

In section \ref{sec:dynamic_programming} a perfect model of the environment was used in order to compute the backup updates.
Model-Free Reinforcement Learning aims to solve the prediction and control problem only through experience borrowing ideas from Dynamic Programming.


\subsection{Temporal-Difference Learning}
\label{subsec:TD}
Temporal Difference Learning (TD) is a \textbf{bootstrapping} method: it learns directly from transitions of incomplete episodes using previous estimates.

\subsubsection{Prediction}
For the prediction problem TD updates its estimate of $v^\pi(s_t)$ only using the next step reward $r_{t+1}$ and its current estimate of $v^\pi(s_{t+1})$.

\begin{equation}
    \label{eq:TD(0)}
    V(S_t) \gets V(S_{t+1}) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
\end{equation}

$R_{t+1} + \gamma V(S_{t+1})$ is called TD \textit{target} and the quantity $\delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$ is called TD \textit{error}.



\subsubsection{Control}

For the Control problem TD algorithms can be divided into two categories:
\begin{enumerate}
    \item On-Policy Learning
    \item Off-Policy Learning
\end{enumerate}

In On Policy Learning the algorithmy directly improves the policy $\pi$ used to collect the samples from the environment while
Off Policy Learning allows to improve a policy $\pi'$ different from the one that interacts with the environment.
\\
One major problem in Model-Free reinforcement learning is the \textit{Exploration-Exploitation} tradeoff:
Should the agent try out new actions to discover their potential rewards or exploit its current knowledge in order to maximize the expected reward?
\\
\begin{definition}
    A soft policy $\pi$ is a policy with $\pi(a|s) > 0$, $\forall s \in \mc{S}$ and $\forall a \in \mc{A}(s)$.
\end{definition}

\begin{definition}
    An $\epsilon$-soft policy $\pi$ is a policy with $\pi(s,a) \ge \frac{\epsilon}{|\mc{A}(s)|}$, $\forall s \in \mc{S}$ and $\forall a \in \mc{A}(s)$ and some $\epsilon > 0$.
\end{definition}

\begin{definition}
    An $\epsilon$-greedy policy $\pi$ is a policy with:
    \begin{equation*}
        \pi(a|s) =
        \begin{cases}
            \frac{\epsilon}{|\mc{A}|} + 1 - \epsilon & \text{if } a = \argmax_{a' \in \mc{A}}Q(s, a') \\
            \frac{\epsilon}{|\mc{A}|}                & \text{otherwise}
        \end{cases}
    \end{equation*}
\end{definition}

The $\epsilon$-greedy policy is a specific example of $\epsilon$-soft policy that is the closest to a greedy policy.
It is the simplest idea for ensuring continual exploration while acting greedily.

\begin{theorem}
    For any $\epsilon$-greedy policy $\pi$, the $\epsilon$-greedy policy $\pi'$ with respect to $q^\pi$ is an improvement.
    \begin{align*}
        q^\pi(s,\pi'(s))    &= \sum_{a \in \mc{A}}\pi'(a|s)q^\pi(s,a) \\
                            &= \frac{\epsilon}{m}\sum_{a \in \mc{A}}q^\pi(s,a) + (1 - \epsilon)\max_{a \in \mc{A}}q^\pi(s,a) \\
                            &\ge \frac{\epsilon}{m}\sum_{a \in \mc{A}}q^\pi(s,a) + (1 - \epsilon)\sum_{a \in \mc{A}}\frac{\pi(a|s) - \frac{\epsilon}{m}}{1 - \epsilon}q^\pi(s,a) \\
                            &= \sum_{a \in \mc{A}}\pi(a|s)q^\pi(s,a) = v^\pi(s)
    \end{align*}
    with $m = |\mc{A}|$. \\
    Therefore from the policy improvement theorem \ref{th:policy_improvement}, $v^{\pi'}(s) \ge v^\pi(s)$.
\end{theorem}

\subsubsection{Sarsa: On-policy TD Control}
Applying the same TD method \eqref{eq:TD(0)} used to estimate $v^\pi$ to the action-value function $q^\pi$ leads to the following update rule:
\begin{equation*}
    Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
\end{equation*}.

This rule uses every element of a quintuple of events, $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that make up a transition from one state-action pair to the next.
This quintuple gives rise to the name \textit{Sarsa} for the algorithm.

It is straightforward to design an on-policy control algorithm based on the Sarsa prediction method. As in all on-policy methods, we continually estimate $q^\pi$
for the behavior policy $\pi$, and at the same time change $\pi$ towards greediness with respect to $q^\pi$.

\begin{algorithm}[H]
    \caption{Sarsa}
    \label{alg:sarsa}
    \begin{algorithmic}[1]
        \STATE Initialize $Q(s,a)$ arbitrarily
        \LOOP
            \STATE Initialize $S$
            \STATE Choose $A$ from $S$ using policy derived from $Q$ (e.g. $\epsilon$-greedy)
            \REPEAT
                \STATE Take action $A$, observe $R$,$S'$
                \STATE Choose $A'$ from $S'$ using policy derived from $Q$ (e.g. $\epsilon$-greedy)
                \STATE $Q(S,A) \gets Q(S,A) + \alpha[R + \gamma Q(S',A') - Q(S, A)]$
                \STATE $S \gets S'$; $A \gets A'$
            \UNTIL{$S$ is terminal}
        \ENDLOOP
    \end{algorithmic}
\end{algorithm}

\subsubsection{Q-Learning: Off-policy TD Control}
One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as \textit{Q-Learning}
\cite{Watkins:1989}.

In this algorithm the learned action-value function $Q$ directly approximates $q^*$, the optimal action-value function, independent of the 
policy being followed:
\begin{equation*}
    Q(S_t,A_t) \gets Q(S_t,A_t) + \alpha [R_{t+1} + \gamma \max_{a \in \mc{A}}Q(S_{t+1}, a) - Q(S_t, A_t)]
\end{equation*}

In this case, the learned action-value function $Q$ directly approximates  $q^*$, the optimal value action-value function thanks to the $\max$ operation.

\begin{algorithm}[H]
    \caption{Q-Learning}
    \label{alg:q-learning}
    \begin{algorithmic}[1]
        \STATE Initialize $Q(s,a)$ arbitrarily
        \LOOP
            \STATE Initialize $S$
            \STATE Choose $A$ from $S$ using policy derived from $Q$ (e.g. $\epsilon$-greedy)
            \REPEAT
                \STATE Take action $A$, observe $R$,$S'$
                \STATE $Q(S,A) \gets Q(S,A) + \alpha[R + \gamma \max_{a \in \mc{A}}Q(S',a) - Q(S, A)]$
                \STATE $S \gets S'$
            \UNTIL{$S$ is terminal}
        \ENDLOOP
    \end{algorithmic}
\end{algorithm}

\subsubsection{Convergence of Sarsa and Q-Learning}
\begin{definition}
    A learning policy is called Greedy in the limit of infinite exploration (GLIE) if it satisfies the following two conditions:
    \begin{enumerate}
        \item All state-action pairs are explored infinitely many times
        \begin{equation*}
            \lim_{k \to \infty}N_k(s,a) = \infty
        \end{equation*}
        \item The policy converges on a greedy policy
        \begin{equation*}
            \lim_{k \to \infty} \pi_k(a|s) = \mathbbm{1}(a = \argmax_{a' \in \mc{A}}Q_k(s,a'))
        \end{equation*}
    \end{enumerate}
\end{definition}

\begin{theorem}
    SARSA converges to the optimal action-value function $Q(s,a) \to Q^*(s,a)$ under the following conditions:
    \begin{enumerate}
        \item GLIE sequence of policies $\pi_t(s,a)$
        \item Robbins-Monro sequence of step-sizes $\alpha_t$
            \begin{align*}
                &\sum_{t=1}^\infty \alpha_t = \infty
                &\sum_{t=1}^\infty \alpha_t^2 < \infty
            \end{align*}
    \end{enumerate}
\end{theorem}

This means that in order for SARSA to converge to $q^*$ we need to decrease $\epsilon$ over time (e.g. $\epsilon = \frac{1}{t}$).
If $\epsilon$ is kept fixed SARSA will converge to the optimal policy among the $\epsilon$-soft policies.
Being Q-Learning an off-policy algorithm that is already improving Q towards $q^*$ there is no need for an $\epsilon$ decay as long as all state-action
pairs are visited infinitely and the learning rate' schedule satisfies the Robbins-Monro conditions.


\subsection{Reinforcement Learning Taxonomy}
% Tutte le distinzioni dei vari algoritmi
This subsection includes a brief overview of the whole Reinforcement Learning taxonomy. \ref{subsec:TD} presented two well known TD tabular algorithms
but often real world scenarios have to be modeled with complex state and action spaces with more dimensions resulting in intractable problems.
%Moreover these spaces could be continuous and tabular methods cannot be applied anymore.

\begin{itemize}
\item \textbf{Function Approximation} \\
When the state space is too big for tabular reinforcement learning approximate solution methods are used instead.
Typically the value function (or action-value function) is parameterized with a weight vector $\mathbf{w} \in \mathbb{R}^d$ and $d \ll |\mc{S}|$:
\begin{equation*}
    \hat v(s, \mathbf{w}) \approx v_\pi(s)
\end{equation*}

For example $\hat v$ might be a linear function in features of the state, with $\mathbf{w}$ the vector of feature weights.
More generally, $\hat v$ might be the function computed by a multi-layer artificial neural network or even the function computed
by a decision tree.
\\

In addition, function approximation allow continous state spaces to be managed without the need for discretization techniques.

\item \textbf{Value-Based Algorithms} \\
All the algorithms that seek to learn the action values and then select actions based on their estimated values are called Value-Based Algorithms.
The temporal difference algorithms presented in \ref{subsec:TD} are all value-based algorithms.
\item \textbf{Policy-Based Algorithms} \\
Differently from Value-Based Algorithms Policy-Based algorithms seek to learn a \textit{parameterized policy} that can select actions without consulting
a value function:
\begin{equation*}
    \pi(a|s,\theta) = \mathbb{P}(A_t = a | S_t = s, \theta_t = \theta)
\end{equation*}

One important consequence of the use of parameterized policies is that they offer practical ways of dealing with large action spaces,
even continuous spaces with an infinite number of actions. Instead of computing the learned probability for each of the many actions,
we instead learn statistics of the probability distribution. For example, the action set might be the real numbers, with actions chosen
from a normal distribution:

\begin{definition}
    A gaussian policy suitable for continuous action space is given by
    \begin{equation}
        \label{eq:gaussian_policy}
        \pi(a|s,\theta) = \frac{1}{\sigma(s, \theta)\sqrt{2 \pi}}\exp\left(-\frac{(a - \mu(s, \theta))^2}{2\sigma(s,\theta)^2}\right)
    \end{equation}
\end{definition}

where $\mu: \mc{S} \times \Theta \to \mathbb{R}$ is the mean function and $\sigma: \mc{S} \times \Theta \to (0, \infty)$ is the standard deviation function.
$\Theta$ is the parameter space of the policy.

A sample from a gaussian policy can be computed as follows:
\begin{equation*}
    a = \mu(s, \theta) + \sigma(s, \theta) \eta
\end{equation*}

drawing $\eta$ from a standard distribution $\mc{N}(0,1)$.



\item \textbf{Actor-Critic Algorithms} \\
A combination of value-based methods and policy-based methods gives rise to Actor-Critic algorithms. Actor critic algorithms
learn a parameterized policy (Actor) and a value function (Critic).
The actor is updated in the direction that maximizes the expected reward using gradients derived from the critic's feedback.
The critic estimates the actions taken by the actor.

\end{itemize}

\subsection{Deep Reinforcement Learning}
In recent years Deep Learning techniques applied to Reinforcement Learning techniques have shown to yield very good results.
The first notable example came from Mnih et al. \cite{DQN} modifying Q-Learning to work with Deep Neural Networks.
At that time, their approach achieved state of the art on multiple Atari Games with no prior knowledge and same hyperparameters for each game. This algorithm, called
\textit{Deep Q-Networks} (DQN) uses an \textit{experience replay buffer} to store played transitions for sampling minibatches.
These minibatches are used to train the Q-Network using Stochastic Gradient Descent with backpropagation on the neural network.

Unfortunately nonlinear function approximators like neural networks come with a theoretical cost:
convergence to an optimum is only guaranteed locally. Despite this limitation, deep reinforcement learning approaches remain the most widely used methods in the field 
due to their empirical success and ability to handle complex, high-dimensional state spaces.


Below, a state of the art algorithm called \textit{Soft Actor-Critic} (SAC) is presented.

\subsubsection{Soft Actor-Critic}
\label{sec:SAC}
SAC \cite{SAC, SAC_modified} is an off policy actor-critic deep RL algorithm based on the maximum entropy framework in which the standard reinforcement learning
objective is augmented with an entropy term:
\begin{equation*}
    \pi^* = \argmax_\pi \sum_t \mathbb{E}_{(s_t, a_t) \sim \rho_\pi}[r(s_t, a_t) + \alpha \mc{H}(\pi(.|s_t))]
\end{equation*}

where $\alpha$ is the temperature parameter that determines the relative importance of the entropy term versus the reward, and thus controls the stochasticity
of the optimal policy.

This objective has shown to improve exploration and improve learning speed over state-of-art methods that optimize the conventional RL objective function.

\begin{definition}
    The soft state-value and action value functions  $v_{\text{soft}}: \mc{S} \to \mathbb{R}$ and $q_{\text{soft}}: \mc{S} \times \mc{A} \to \mathbb{R}$ are defined by:
    \begin{align*} 
        &v_\text{soft}^\pi(s_t) = \mathbb{E}_{a_t \sim \pi }[q^\pi(s_t, a_t) - \alpha \log \pi(a_t | s_t)] \\
        &q_\text{soft}^\pi(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \mathit{p}}[v_\text{soft}^\pi(s_{t+1})]
    \end{align*}
\end{definition}

\textit{Soft Actor-Critic} is based on the soft policy iteration algorithm, an adaptation of policy iteration algorithm that includes the entropy term in the value function
and guarantees convergence to an optimal policy.

In the soft policy iteration, the policy is updated towards the exponential of the soft Q-function. This particular choice of update can be guaranteed
to result in an improved policy in terms of its soft values:

\begin{equation}
    \label{eq:soft_policy_improvement}
    \pi_\text{new} = \argmin_{\pi' \in \Pi} D_{KL} \left(\pi'(.|s_t) \Big\Vert \frac{\exp(\frac{1}{\alpha}Q^\text{old}(s_t, .))}{Z^{\pi_{\text{old}}(s_t)}}\right),
\end{equation}

where the partition function $Z^\pi_\text{old}(s_t)$ normalizes the distribution, and while it is intractable in general, it does not contribute to the gradient with respect to the new policy and thus can be ignored.

\textit{Soft Actor-Critic} uses function approximators for both the soft Q-function and the policy, and instead of running evaluation and improvement to convergence
like soft policy iteration, alternates between optimizing both networks with stochastic gradient descent. The soft Q-function can be modeled
as expressive neural networks, and the policy as a Gaussian distribution \eqref{eq:gaussian_policy} with mean and covariance given by neural networks 

The soft Q-function parameters can be trained to minimize the soft Bellman residual through stochastic gradients:

\begin{equation*}
    J_Q(\theta) = \mathbb{E}_{(s_t,a_t) \sim \mathcal{D}} \left[ \frac{1}{2} \left( Q_\theta(s_t, a_t) - \left( r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p} [V_{\bar{\theta}}(s_{t+1})] \right) \right)^2 \right]
\end{equation*}

where $\mc{D}$ is the replay buffer storing past experience. In whis way the reinforcement learning objective is approximated
as a supervised learning objective over a continually changing buffer of past experience.

\begin{equation*}
    \hat{\nabla}_\theta J_Q(\theta) = \nabla_\theta Q_\theta(a_t, s_t) \left( Q_\theta(s_t, a_t) - \left( r(s_t, a_t) + \gamma \left( Q_{\bar{\theta}}(s_{t+1}, a_{t+1}) - \alpha \log (\pi(a_{t+1} | s_{t+1})) \right) \right) \right)
\end{equation*}

The update makes use of a target Q-Function with parameters $\bar{\theta}$ that are obtained as an exponential moving average of the soft Q-function weights, which has shown to stabilize training \cite{DQN}.
Finally, the policy parameters can be learned by directly minimizing the expected KL-Divergence in \eqref{eq:soft_policy_improvement}:

\begin{equation}
    \label{eq:sac_actor_loss}
    J_\pi(\phi) = \mathbb{E}_{s_t \sim \mc{D}} \left[\mathbb{E}_{a_t \sim \pi_\phi} \left[\alpha \log{\pi_\phi(a_t|s_t)} - Q_\theta(s_t, a_t)\right]\right]
\end{equation}

As the Q-function is differentiable it is convenient to apply the reparameterization trick to the policy in order to minimize $J_\pi$:

\begin{equation*}
    a_t = f_\phi(\epsilon_t; s_t)
\end{equation*}

where $\epsilon_t$ is an input noise, sampled from some fixed distribution, such as a spherical Gaussian.

The objective function in \eqref{eq:sac_actor_loss} can be rewritten as
\begin{equation}
    \label{eq:sac_actor_loss2}
    J_\pi(\phi) = \mathbb{E}_{s_t \sim \mc{D}, \epsilon_t \sim \mc{N}}\left[\alpha \log{\pi_\phi(f_\phi(\epsilon_t;s_t)|s_t)} - Q_\theta(s_t, f_\phi(\epsilon_t;s_t))\right]
\end{equation}

where $\pi_\phi$ is implicitly defined in terms of $f_\phi$. The gradient of equation \eqref{eq:sac_actor_loss2} is then approximated with

\begin{equation*}
    \hat{\nabla}_\phi J_\pi(\phi) = \nabla_\phi \alpha \log{\pi_\phi(a_t|s_t)} + \left(\nabla_{a_t} \alpha \log{\pi_\phi(a_t|s_t)} - \nabla_{a_t}Q_\theta(s_t, a_t)\right) \nabla_\phi f_\phi(\epsilon_t;s_t)
\end{equation*}
where $a_t$ is evaluated at $f_\phi(\epsilon_t;s_t)$.

SAC also employs an automatic entropy adjustment algorithm that finds the optimal entropy coefficient during training.
Also, it makes use of two soft Q functions to mitigate positive bias in the policy improvement step that is known to degrade performance of value based methods \cite{Hasselt2010, Fujimoto2018}.
In particular, we parameterize two soft Q-functions, with parameters $\theta_i$ , and train them independently to optimize $J_Q(\theta_i)$.
For further details refer to \cite{SAC_modified}.



\begin{algorithm}[H]
    \caption{Soft Actor-Critic (SAC) Algorithm}
    \begin{algorithmic}[1]
    \STATE \textbf{Initialize} policy parameters $\phi$, Q-function parameters $\theta_1$, $\theta_2$
    \STATE \textbf{Initialize} replay buffer $\mathcal{D}$
    \STATE \textbf{Set} target Q-function weights: $\bar{\theta}_i \leftarrow \theta_i$ for $i \in \{1, 2\}$
    \FOR{each iteration}
        \FOR{each environment step}
            \STATE Sample action $a_t \sim \pi_\theta(a_t|s_t)$
            \STATE Execute action $a_t$ in the environment
            \STATE Observe next state $s_{t+1}$, reward $r_t$
            \STATE Store $(s_t, a_t, r_t, s_{t+1})$ in replay buffer $\mathcal{D}$
        \ENDFOR
        \FOR{each gradient step}
            \STATE Sample a batch of transitions $(s, a, r, s')$ from $\mathcal{D}$
            \STATE $\theta_i \leftarrow \theta_i - \lambda_Q \hat{\nabla}_{\theta_i}J_Q(\theta_i)$ for $i \in \{1, 2\}$
            \STATE $\phi \leftarrow \phi - \lambda_\pi \hat{\nabla}_\phi J_\pi(\phi)$
            \STATE $\bar{\theta}_i \leftarrow \tau \theta_i + (1 - \tau) \bar{\theta}_i$ for $i \in \{1, 2\}$ 
            
        \ENDFOR
    \ENDFOR
    \end{algorithmic}
    \end{algorithm}















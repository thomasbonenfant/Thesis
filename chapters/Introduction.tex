\chapter*{Introduction}
\label{ch:Introduction}
% Sequential decision making is a process where an agent makes a series of decisions over time, each influencing future states and decisions. 
% This process involves dynamically selecting actions based on current and past information to achieve a long-term objective.
% Several techniques exist to find an agent that achieves this long term objective: the techniques employed to find an agent
% able to achieve this objective usually depends on our knowledge of the environment: perfect knowledge enable us to find an exact
% solution directly, although very expensive to compute, or iteratively, using the dynamic programming framework. 
% Generally however, a perfect model of the environment the agent is taking decision in is not known, and therefore we engage the problem
% using the reinforcement learning framework. Reinforcement learning enables an agent to learn from trial and error interacting
% with the environment that outputs its state and a reward. Applying this trial and error in some applications can become problematic:
% robotic control in real-world with this approach can lead to safety issues and unwanted damages to the robotic system and its environment.
% This is why in some cases reinforcement learning is first applied during a simulation of the environment and next deployed on a real world
% system. This transfer however suffers from performance drops and still safety and damage issues as the model of the environment
% does not take into account real-world problems. This gap between simulation and real world is called reality gap.

Sequential decision making involves making a series of decisions over time, with each decision influencing future states and subsequent decisions. 
Actions are dynamically selected based on current and past information to achieve a long-term objective. 
Several techniques are available to achieve this long-term objective, and the choice of technique typically depends on the level of 
knowledge about the environment.

When perfect knowledge of the environment is available, a solutions can be found via dynamic programming or linear programming techniques. 
However, in most cases, a perfect model of the environment is not available, necessitating the use of the reinforcement learning framework.

Reinforcement learning enables learning from trial and error by interacting with the environment, 
which provides states and rewards. However, applying this trial-and-error approach can be problematic in some applications. 
For example, in real-world robotic control, this approach can lead to safety issues and unwanted damage to the robotic system and its environment. 
Therefore, reinforcement learning is often first applied in a simulated environment before being deployed in the real world.

This transfer from simulation to the real world often suffers from performance drops and still presents safety and damage issues because the simulation 
model may not be accurate or may not account for noise and other real-world problems. 
This discrepancy between simulation and the real world is known as the reality gap.
\section*{Motivation}

Closing the reality gap is a crucial research direction in achieving true embodied intelligence. 
To attain this goal, multiple factors must be considered: in real-world scenarios, 
we encounter uncertainties stemming from sensor noise, modeling errors, and delays. 
Additionally, agents deployed on real robotic systems must operate without damaging their components, the environment, or causing harm to people.

The game of air hockey presents a complex and dynamic challenge where a robotic player must be highly reactive, 
adhere to constraints, and exhibit long-term planning capabilities. 
This scenario compels engineers to push the boundaries of robotic capabilities while ensuring safety.

With these challenges in mind, the Robot Air Hockey competition was organized. 
This competition serves as a platform for researchers in the field of robot learning to engage with a realistic robotic task. 
Teams participating in the competition design and build their air hockey agents, 
competing against each other in various subtasks (in simulation) and ultimately in full games (both in simulation and in the real world).

To be successful, an agent must solve multiple sub-problems, such as scoring goals against an opponent, 
effectively defending its own goal, and devising winning strategies. 
The inherent sub-problem composition of air hockey inspires this work, where a hierarchical approach to the game is investigated. 
Reinforcement learning techniques are applied to each sub-problem that contributes to a complete air hockey match. 
These specialized agents are then integrated into a unified high-level agent that continuously selects the optimal low-level agent to deploy during the match.

\section*{Goal}
% The goal of this work is to develop an agent capable of playing entire games without violating constraints. The agent has to control a general purpose manipulator
% and achieve the best possible performance. 
The goal of this work is to develop an agent capable of playing entire air hockey games while adhering to all operational constraints. Specifically, the agent must control a general-purpose manipulator, achieving the best possible performance
in terms both of offense and defense.
It is crucial that the agent maintains these performance levels without violating any constraints, 
ensuring that no drop in performance or damage concerns arise when deployed on a real robot. The agent must:
\begin{itemize}
    \item react quickly and accurately to the puck's movements.
    \item Maintain strategic positioning and make decisions that balance offensive and defensive play.
    \item Operate within safety margins, ensuring that the manipulator does not exceed its physical limits or damage constraints.
\end{itemize}

\section*{Thesis Structure}
% The thesis starts with a Preliminary background, introducing the main theory of Reinforcement Learning and presents a start-of-art Deep Reinforcement Learning algorithms
% used for continuous control as well as a brief overview of the Hierarchical reinforcement learning frameworks. 
% Next it follows with a Related works chapter briefly introducing a trajectory optimization algorithm used for robotics control and describes the main challenges
% found in applying reinforcement learning techniques to robotics.

% The next chapter describes the Air Hockey competition, its rules and the provided framework. The methodology chapter describes the agents that were used during
% the competition and an Experimental Results chapter lists the results obtained with an additional section describing improvements made after the competition ended.
The thesis is structured as follows:
\begin{enumerate}
    \item \textbf{Preliminary Background}:
    This section introduces the main theories of Reinforcement Learning and presents state-of-the-art Deep Reinforcement Learning algorithms 
    used for continuous control, as well as a brief overview of hierarchical reinforcement learning frameworks.
    \item \textbf{Related Works}:
    This chapter reviews existing literature on trajectory optimization algorithms used for robotics control and describes the main challenges in applying reinforcement learning techniques to robotics.
    \item \textbf{Air Hockey Competition}:
    This chapter describes the competition, its rules, and the provided framework.
    \item \textbf{Methodology}:
    This chapter details the design and implementation of the agents used during the competition, including the hierarchical approach and reinforcement learning techniques applied.
    \item \textbf{Experimental Results}:
    This chapter presents the results obtained during the competition and discusses the improvements made post-competition.
    \item \textbf{Conclusions and future developments}

\end{enumerate}
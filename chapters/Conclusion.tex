\chapter{Conclusions and future developments}
\label{ch:conclusions}%
% In this work, the air hockey competition was described.  

% In this work, reinforcement learning was successfully applied to the air hockey task using a general-purpose manipulator.

% The agent was able to play entire matches without violating too many constraints, thanks to the ATACOM algorithm. 
% The SAC method outperformed the rule-based controller for the hitting task, achieving a higher success rate. 
% The rule-based switcher effectively selected the appropriate policy at the right time, with its parameters slightly optimized via PGPE.

This thesis explored the development and implementation of a hierarchical reinforcement learning agent designed to compete in 
the air hockey competition, which was initiated to address the reality gap problem in reinforcement learning. 
The competition served as a platform to test and refine approaches that bridge the gap between simulated environments 
and real-world applications, ensuring that the solutions developed are robust and effective in practical scenarios.

The hierarchical agent developed in this work consists of specialized lower-level policies, 
each tailored to handle specific sub-tasks within the game of air hockey. 
These sub-policies were integrated under a rule-based high-level policy, responsible for selecting the most appropriate 
specialized agent during gameplay. This high-level policy was trained using PGPE, enabling effective exploration and optimization despite the non-differentiable nature of the policy.

Additionally, the ATACOM algorithm was employed to ensure that the agent operated within the defined constraints.

The results demonstrated that the hierarchical structure allowed the agent to perform effectively in the competition, 
showcasing improved adaptability and performance.


\section{Future works}
% The developed rule-based switcher currently uses a few simple parametric rules. 
% Future work could involve implementing a more complex switcher with additional rules that also take the opponent's actions into account.

% An other strategy could be to apply an end-to-end hierarchical reinforcement learning approach, such as HIRO \cite{HIRO}, where a single low-level policy receives from the high-level
% policy a vector to determine the desired behavior.

% Since air hockey is a multi-agent setting, investigating a self-play learning strategy could also be beneficial. 
% This approach could help the agent improve by continuously adapting to and learning from interactions with itself or other agents.

The developed rule-based switcher currently uses a few simple parametric rules. 
Future work could involve implementing a more complex switcher with additional rules that also take the opponent's actions into account. 
This enhancement would allow the agent to make more informed decisions based on the dynamic behavior of its opponent, 
leading to improved performance and adaptability in competitive scenarios.

Another strategy could be to apply an end-to-end hierarchical reinforcement learning approach, such as HIRO \cite{HIRO}. 
In this approach, a single low-level policy receives a latent space vector from the high-level policy, determining the desired behavior. 
This method could streamline the decision-making process and potentially yield more efficient and effective learning, 
as the high-level policy provides structured guidance to the low-level policy.

Given that air hockey is inherently a multi-agent setting, investigating a self-play learning strategy could also be beneficial. 
Self-play allows the agent to improve by continuously adapting to and learning from interactions with itself or other agents. 
This approach can foster the development of robust strategies, as the agent is exposed to a wide range of behaviors and tactics, 
leading to a more comprehensive understanding of the game dynamics.

